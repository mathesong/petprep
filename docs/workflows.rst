.. include:: links.rst

===========================
Processing pipeline details
===========================
*PETPrep* adapts its pipeline depending on what data and metadata are
available and are used as the input.
Certain processing steps will run only when the required metadata is
available in the input dataset.

A (very) high-level view of the simplest pipeline (for a single dataset with only
a single tracer and single baseline)
is presented below:

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.tests import mock_config
    from petprep.workflows.base import init_single_subject_wf
    with mock_config():
        wf = init_single_subject_wf('01')

.. note::

   Each node in this workflow is either a processing node or a sub-workflow.
   Several conventions appear in this workflow that will be apparent throughout
   PETPrep.

   * ``inputnode``\s are special nodes that provide the runtime-generated inputs
     to a workflow. These are like function "arguments". There are corresponding
     ``outputnode``\s in most other workflows, which are like function return
     values.
   * Workflows end with ``_wf``, and are generated by a function of the form
     ``init_{workflow}_wf``.
     For example, ``anat_preproc_wf`` is a sub-workflow that is generated by the
     :func:`~smriprep.workflows.anatomical.init_anat_preproc_wf` (see below).
     Because each task and run of functional data is processed separately,
     :func:`~petprep.workflows.pet.base.init_pet_wf` names the
     resulting workflows using input parameters, resulting in
     ``func_preproc_task_{task}_run_{run}_wf``.
   * Datasinks begin with ``ds_``, and save files to the output directory.
     This is in contrast to most nodes, which save their outputs to the working
     directory. ``ds_report_`` nodes indicate that the node is saving text and
     figures for generating reports, rather than processed data.
   * When a name appears in parentheses, such as ``(reports)`` in ``about (reports)``
     it is the module where the interface is defined. In this case, ``about``
     is an :class:`~petprep.interfaces.reports.AboutSummary`, found in the
     :mod:`petprep.interfaces.reports` module.

Preprocessing of structural MRI
-------------------------------
The anatomical sub-workflow begins by constructing an average image by
conforming all found T1w images to RAS orientation and
a common voxel size, and, in the case of multiple images, averages them into a
single reference template (see `Longitudinal processing`_).

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from niworkflows.utils.spaces import Reference, SpatialReferences
    from smriprep.workflows.anatomical import init_anat_preproc_wf
    spaces=SpatialReferences([
        ('MNI152Lin', {}),
        ('fsaverage', {'density': '10k'}),
        ('T1w', {}),
        ('fsnative', {})
    ])
    spaces.checkpoint()
    wf = init_anat_preproc_wf(
        bids_root='.',
        freesurfer=True,
        hires=True,
        longitudinal=False,
        omp_nthreads=1,
        output_dir='.',
        skull_strip_mode='force',
        skull_strip_template=Reference('MNI152NLin2009cAsym'),
        spaces=spaces,
        skull_strip_fixed_seed=False,
        t1w=['sub-01/anat/sub-01_T1w.nii.gz'],
        t2w=[],
        msm_sulc=True,
        precomputed={},
    )

.. important::

    Occasionally, openly shared datasets may contain preprocessed anatomical images
    as if they are unprocessed.
    In the case of brain-extracted (skull-stripped) T1w images, attempting to perform
    brain extraction again will often have poor results and may cause *PETPrep* to crash.
    *PETPrep* can attempt to detect these cases using a heuristic to check if the
    T1w image is already masked.
    This must be explicitly requested with ``---skull-strip-t1w auto``.
    If this heuristic fails, and you know your images are skull-stripped, you can skip brain
    extraction with ``--skull-strip-t1w skip``.
    Likewise, if you know your images are not skull-stripped and the heuristic incorrectly
    determines that they are, you can force skull stripping with ``--skull-strip-t1w force``,
    which is the current default behavior.

See also *sMRIPrep*'s
:py:func:`~smriprep.workflows.anatomical.init_anat_preproc_wf`.

.. _t1preproc_steps:

Brain extraction, brain tissue segmentation and spatial normalization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Then, the T1w reference is skull-stripped using a Nipype implementation of
the ``antsBrainExtraction.sh`` tool (ANTs), which is an atlas-based
brain extraction workflow:

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from niworkflows.anat.ants import init_brain_extraction_wf
    wf = init_brain_extraction_wf()


Once the brain mask is computed, FSL ``fast`` is utilized for brain tissue segmentation.

PETPrep includes a single figure overlaying the brain mask (red), and tissue boundaries
(blue = gray/white; magenta = tissue/CSF):

.. figure:: _static/sub-01_dseg.svg

    Brain extraction and segmentation report

Finally, spatial normalization to standard spaces is performed using ANTs' ``antsRegistration``
in a multiscale, mutual-information based, nonlinear registration scheme.
See :ref:`output-spaces` for information about how standard and nonstandard spaces can
be set to resample the preprocessed data onto the final output spaces.

.. figure:: _static/T1MNINormalization.svg

    Animation showing spatial normalization of T1w onto the ``MNI152NLin2009cAsym`` template.

Cost function masking during spatial normalization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
When processing images from patients with focal brain lesions (e.g., stroke, tumor
resection), it is possible to provide a lesion mask to be used during spatial
normalization to standard space [Brett2001]_.
ANTs will use this mask to minimize warping of healthy tissue into damaged
areas (or vice-versa).
Lesion masks should be binary NIfTI images (damaged areas = 1, everywhere else = 0)
in the same space and resolution as the T1w image, and use the ``_roi`` suffix,
for example, ``sub-001_label-lesion_roi.nii.gz``.
This file should be placed in the ``sub-*/anat`` directory of the BIDS dataset
to be run through *PETPrep*.
Because lesion masks are not currently part of the BIDS specification, it is also necessary to
include a ``.bidsignore`` file in the root of your dataset directory. This will prevent
`bids-validator <https://github.com/bids-standard/bids-validator#bidsignore>`_ from complaining
that your dataset is not valid BIDS, which prevents *PETPrep* from running.
Your ``.bidsignore`` file should include the following line::

  *lesion_roi.nii.gz

.. note::

   The lesion masking instructions in this section predate the release of BIDS Derivatives.
   As of BIDS 1.4.0, the recommended naming convention is::

       manual_masks/
       └─ sub-001/
          └─ anat/
             ├─ sub-001_desc-tumor_mask.nii.gz
             └─ sub-001_desc-tumor_mask.json

   In an upcoming version of PETPrep, we will search for lesion masks as pre-computed
   derivatives. Until this is supported, we will continue to look for the ``_roi`` suffix.

Longitudinal processing
~~~~~~~~~~~~~~~~~~~~~~~
In the case of multiple T1w images (across sessions and/or runs), T1w images are
merged into a single template image using FreeSurfer's `mri_robust_template`_.
This template may be *unbiased*, or equidistant from all source images, or
aligned to the first image (determined lexicographically by session label).
For two images, the additional cost of estimating an unbiased template is trivial,
but aligning three or more images is too expensive to justify being the default behavior.
For consistency, in the case of multiple images, *PETPrep* constructs
templates aligned to the first image, unless passed the ``--longitudinal``
flag, which forces the estimation of an unbiased template.

.. note::

    The preprocessed T1w image defines the ``T1w`` space.
    In the case of multiple T1w images, this space may not be precisely aligned
    with any of the original images.
    Reconstructed surfaces and functional datasets will be registered to the
    ``T1w`` space, and not to the input images.

.. _workflows_surface:

Surface preprocessing
~~~~~~~~~~~~~~~~~~~~~
*PETPrep* uses FreeSurfer_ to reconstruct surfaces from T1w/T2w
structural images.
If enabled, several steps in the *PETPrep* pipeline are added or replaced.
All surface preprocessing may be disabled with the ``--fs-no-reconall`` flag.

.. note::
    Surface processing will be skipped if the outputs already exist.

    In order to bypass reconstruction in *PETPrep*, place existing reconstructed
    subjects in ``<output dir>/sourcedata/freesurfer`` prior to the run, or specify
    an external subjects directory with the ``--fs-subjects-dir`` flag.
    *PETPrep* will perform any missing ``recon-all`` steps, but will not perform
    any steps whose outputs already exist.


If FreeSurfer reconstruction is performed, the reconstructed subject is placed in
``<output dir>/sourcedata/freesurfer/sub-<subject_label>/`` (see :ref:`fsderivs`).

Surface reconstruction is performed in three phases.
The first phase initializes the subject with T1w and T2w (if available)
structural images and performs basic reconstruction (``autorecon1``) with the
exception of skull-stripping.
Skull-stripping is skipped since the brain mask :ref:`calculated previously
<t1preproc_steps>` is injected into the appropriate location for FreeSurfer.
For example, a subject with only one session with T1w and T2w images
would be processed by the following command::

    $ recon-all -sd <output dir>/freesurfer -subjid sub-<subject_label> \
        -i <bids-root>/sub-<subject_label>/anat/sub-<subject_label>_T1w.nii.gz \
        -T2 <bids-root>/sub-<subject_label>/anat/sub-<subject_label>_T2w.nii.gz \
        -autorecon1 \
        -noskullstrip

The second phase imports the brainmask calculated in the
`Preprocessing of structural MRI`_ sub-workflow.
The final phase resumes reconstruction, using the T2w image to assist
in finding the pial surface, if available.
See :py:func:`~smriprep.workflows.surfaces.init_autorecon_resume_wf` for
details.

Reconstructed white and pial surfaces are included in the report.

.. figure:: _static/reconall.svg

    Surface reconstruction (FreeSurfer)

If T1w voxel sizes are less than 1mm in all dimensions (rounding to nearest
.1mm), `submillimeter reconstruction`_ is used, unless disabled with
``--no-submm-recon``.

If T2w or FLAIR images are available, and you do not want them included in
FreeSurfer reconstruction, use ``--ignore t2w`` or ``--ignore flair``,
respectively.

``lh.midthickness`` and ``rh.midthickness`` surfaces are created in the subject
``surf/`` directory, corresponding to the surface half-way between the gray/white
boundary and the pial surface.
The ``smoothwm``, ``midthickness``, ``pial`` and ``inflated`` surfaces are also
converted to GIFTI_ format and adjusted to be compatible with multiple software
packages, including FreeSurfer and the `Connectome Workbench`_.

.. note::
    GIFTI surface outputs are aligned to the FreeSurfer ``T1.mgz`` image, which
    may differ from the T1w space in some cases, to maintain compatibility
    with the FreeSurfer directory.
    Any measures sampled to the surface take into account any difference in
    these images.

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from smriprep.workflows.surfaces import init_surface_recon_wf
    wf = init_surface_recon_wf(
        omp_nthreads=1, hires=True, precomputed={}, fs_no_resume=False,
    )

See also *sMRIPrep*'s
:py:func:`~smriprep.workflows.surfaces.init_surface_recon_wf`

Refinement of the brain mask
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Typically, the original brain mask calculated with ``antsBrainExtraction.sh``
will contain some inaccuracies including small amounts of MR signal from
outside the brain.
Based on the tissue segmentation of FreeSurfer (located in ``mri/aseg.mgz``)
and only when the :ref:`Surface Processing <workflows_surface>` step has been
executed, *PETPrep* replaces the brain mask with a refined one that derives
from the ``aseg.mgz`` file as described in
:py:class:`~niworkflows.interfaces.freesurfer.RefineBrainMask`.

PET preprocessing
------------------
*PETPrep* performs a series of steps to preprocess :abbr:`PET (positron emission tomography)`
data. Broadly, these are split into fit and transform stages. Stage 1 simultaneously
estimates head motion and the reference image.

The following figures show the overall workflow graph and the ``pet_fit_wf``
subgraph:

:py:func:`~petprep.workflows.pet.base.init_pet_wf`

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.tests import mock_config
    from petprep import config
    from petprep.workflows.pet.base import init_pet_wf
    with mock_config():
        pet_file = config.execution.bids_dir / 'sub-01' / 'pet' \
            / 'sub-01_task-mixedgamblestask_run-01_pet.nii.gz'
        wf = init_pet_wf(pet_series=[str(pet_file)])

.. _pet_fit:

:py:func:`~petprep.workflows.pet.fit.init_pet_fit_wf`

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.tests import mock_config
    from petprep import config
    from petprep.workflows.pet.fit import init_pet_fit_wf
    with mock_config():
        pet_file = config.execution.bids_dir / 'sub-01' / 'pet' \
            / 'sub-01_task-mixedgamblestask_pet.nii.gz'
        wf = init_pet_fit_wf(pet_series=[str(pet_file)])

Preprocessing of :abbr:`PET (positron emission tomography)` files is
split into multiple sub-workflows described below.

.. _pet_ref:

PET reference image estimation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.reference.init_raw_petref_wf`

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.pet.reference import init_raw_petref_wf
    wf = init_raw_petref_wf()

This workflow estimates a reference image for a
:abbr:`PET (positron emission tomography)` series as follows:
When T1-saturation effects ("dummy scans" or non-steady state volumes) are
detected, they are averaged and used as reference due to their
superior tissue contrast.
Otherwise, a median of motion corrected subset of volumes is used.

This reference is used for :ref:`head-motion estimation <pet_hmc>`.

For the :ref:`registration workflow <pet_reg>`, the reference image is
either the above described reference image or a single-band reference,
if one is found in the input dataset.
In either case, this image is contrast-enhanced and skull-stripped
(see :py:func:`~niworkflows.func.util.init_enhance_and_skullstrip_bold_wf`).
If fieldmaps are present, the skull-stripped reference is corrected
prior to registration.

.. figure:: _static/sub-01_task-balloonanalogrisktask_run-1_desc-rois_bold.svg

    The red contour shows the brain mask estimated for a PET reference volume.
    The blue and magenta contours show the tCompCor and aCompCor masks,
    respectively. (See :ref:`pet_confounds`, below.)

.. _pet_hmc:

Head-motion estimation
~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.hmc.init_pet_hmc_wf`

.. workflow::
    :graph2use: colored
    :simple_form: yes

    from petprep.workflows.pet import init_pet_hmc_wf
    wf = init_pet_hmc_wf(
        mem_gb=1,
        omp_nthreads=1)

Using the previously :ref:`estimated reference scan <pet_ref>`,
a robust-template approach estimates head motion.
All frames are aligned to one another with FreeSurfer's
``mri_robust_template`` and ``mri_robust_register`` to create a
within-run template and compute rigid-body transforms for every
:abbr:`PET (positron emission tomography)` volume.
The resulting transforms and the six rotation and translation
parameters for each time-step are passed on to the
:ref:`confounds workflow <pet_confounds>`.

The smoothing kernel width and onset of motion estimation can be
customized via the :option:`--hmc-fwhm` and :option:`--hmc-start-time`
command line options. By default, PETPrep initializes registration with
the frame showing the highest tracer uptake after this start time. An
explicit zero-based frame index can be provided with
:option:`--hmc-init-frame`. Adding :option:`--hmc-init-frame-fix` keeps the chosen
frame fixed during robust template estimation and disables iterations to
reduce runtime. A 10 mm FWHM Gaussian is applied and estimation begins at
120 s unless otherwise specified.

Pre-processed PET in native space
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.fit.init_pet_native_wf`

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.tests import mock_config
    from petprep import config
    from petprep.workflows.pet.fit import init_pet_native_wf
    with mock_config():
        pet_file = config.execution.bids_dir / 'sub-01' / 'pet' \
            / 'sub-01_task-mixedgamblestask_run-01_pet.nii.gz'
        wf = init_pet_native_wf(pet_series=[str(pet_file)])

A new *preproc* :abbr:`PET (positron emission tomography)` series is generated
from the original data in the original space.
All volumes in the :abbr:`PET (positron emission tomography)` series are
resampled in their native space by concatenating the mappings found in previous
correction workflows (:abbr:`HMC (head-motion correction)`)
for a one-shot interpolation process.
Interpolation uses a Lanczos kernel.

.. _pet_reg:

PET to T1w registration
~~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.registration.init_pet_reg_wf`

.. workflow::
    :graph2use: hierarchical
    :simple_form: yes

    from petprep.workflows.pet.registration import init_pet_reg_wf
    wf = init_pet_reg_wf(
        omp_nthreads=1,
        pet2anat_dof=6,
        mem_gb=3,
        use_robust_register=False,
    )

The PET reference volume is aligned to the skull-stripped anatomical image
using FreeSurfer's ``mri_coreg`` with the number of degrees of freedom set via
the :option:`--pet2anat-dof` flag. The resulting affine is converted to ITK
format for downstream application, along with its inverse.

If co-registration proves challenging, the :option:`--pet2anat-robust` flag
switches the workflow to FreeSurfer's ``mri_robust_register`` with an NMI cost function and restricted
to rigid-body (6 dof) transforms. This method is more robust to large initial misalignments.

Resampling PET runs onto standard spaces
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.apply.init_pet_volumetric_resample_wf`

.. workflow::
    :graph2use: colored
    :simple_form: yes

    from petprep.workflows.pet.apply import init_pet_volumetric_resample_wf
    wf = init_pet_volumetric_resample_wf(
        mem_gb={'resampled': 1},
    )

This sub-workflow concatenates the transforms calculated upstream (see
`Head-motion estimation`_, `Susceptibility Distortion Correction (SDC)`_ --if
fieldmaps are available--, `EPI to T1w registration`_, and an anatomical-to-standard
transform from `Preprocessing of structural MRI`_) to map the
:abbr:`EPI (echo-planar imaging)`
image to the standard spaces given by the ``--output-spaces`` argument
(see :ref:`output-spaces`).
It also maps the T1w-based mask to each of those standard spaces.

Transforms are concatenated and applied all at once, with one interpolation (Lanczos)
step, so as little information is lost as possible.

The output space grid can be specified using modifiers to the ``--output-spaces``
argument.

EPI sampled to FreeSurfer surfaces
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.resampling.init_pet_surf_wf`

.. workflow::
    :graph2use: colored
    :simple_form: yes

    from petprep.workflows.pet import init_pet_surf_wf
    wf = init_pet_surf_wf(
        mem_gb=1,
        surface_spaces=['fsnative', 'fsaverage5'],
        medial_surface_nan=False,
        metadata={},
        output_dir='.',
    )

If FreeSurfer processing is enabled, the motion-corrected functional series
(after single shot resampling to T1w space) is sampled to the
surface by averaging across the cortical ribbon.
Specifically, at each vertex, the segment normal to the white-matter surface, extending to the pial
surface, is sampled at 6 intervals and averaged.

Surfaces are generated for the "subject native" surface, as well as transformed to the
``fsaverage`` template space.
All surface outputs are in GIFTI format.

HCP Grayordinates
~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.resampling.init_pet_fsLR_resampling_wf`

.. workflow::
    :graph2use: orig
    :simple_form: yes

    from petprep.workflows.pet.resampling import init_pet_fsLR_resampling_wf
    wf = init_pet_fsLR_resampling_wf(
        grayord_density='92k',
        omp_nthreads=1,
        mem_gb=1,
    )

If CIFTI output is enabled, the motion-corrected functional timeseries (in T1w space) is
resampled onto the subject-native surface, optionally using the `HCP Pipelines_`'s
"goodvoxels" masking method to exclude voxels with local peaks of temporal variation.
After dilating the surface-sampled time series to fill sampling holes, the result is
resampled to the ``fsLR`` mesh (with the left and right hemisphere aligned).
These workflows make use of various `Connectome Workbench`_ functions.
These surfaces are then combined with corresponding volumetric timeseries to create a
CIFTI-2 file.

Segmentation workflows
~~~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.segmentation.init_segmentation_wf`

*PETPrep* ships with optional segmentation routines that can be selected via
the ``--seg`` command-line argument. Supported values are ``gtm`` (the default),
``brainstem``, ``thalamicNuclei``, ``hippocampusAmygdala``, ``wm``, ``raphe`` and
``limbic``.

These workflows rely on pretrained segmentation models distributed with
``petprep.data.segmentation``. The first time a particular model is requested it
will be automatically downloaded to the *PETPrep* cache directory, so ensure
sufficient disk space is available. Each segmentation produces a labeled NIfTI
image ``seg-<seg>_dseg.nii.gz`` and a TSV table of region volumes
``seg-<seg>_morph.tsv`` saved under the ``anat/`` derivatives folder.

Reference masks generated via ``--ref-mask-name`` create a similar
``label-<name>_desc-ref_morph.tsv`` file. These TSVs share the same columns as the
segmentation morph tables: ``index``, ``name`` and ``volume-mm3``.

For example, the raphe segmentation can be enabled with::

   petprep run /data/bids /data/out --seg raphe

Resulting outputs will appear in ``sub-*/anat/`` alongside the anatomical
derivatives and can be used in downstream analyses.

.. _pet_confounds:

Confounds estimation
~~~~~~~~~~~~~~~~~~~~
:py:func:`~petprep.workflows.pet.confounds.init_pet_confs_wf`

.. workflow::
    :graph2use: colored
    :simple_form: yes

    from petprep.workflows.pet.confounds import init_pet_confs_wf
    wf = init_pet_confs_wf(
        name="discover_wf",
        mem_gb=1,
        regressors_dvars_th=1.5,
        regressors_fd_th=0.5,
    )

Given a motion-corrected PET, a brain mask, estimated motion parameters and a
segmentation, the `discover_wf` sub-workflow calculates potential
confounds per volume.

Calculated confounds include the mean global signal, mean tissue class signal,
tCompCor, aCompCor, Frame-wise Displacement, 6 motion parameters, DVARS, and
spike regressors.

Partial volume correction
~~~~~~~~~~~~~~~~~~~~~~~~

:py:func:`~petprep.workflows.pet.pvc.init_pet_pvc_wf`

.. workflow::
    :graph2use: colored
    :simple_form: yes

    from petprep.workflows.tests import mock_config
    from petprep.workflows.pet.pvc import init_pet_pvc_wf

    with mock_config():
        wf = init_pet_pvc_wf(
            name="pvc_wf",
            tool="PETPVC",
            method="GTM"
        )

Partial volume correction (PVC) is a process that attempts to correct for the partial 
volume effects that occur when a single voxel contains multiple tissue types. This is 
particularly important in PET imaging, where the signal from different tissues (e.g., 
gray matter, white matter, and cerebrospinal fluid) can overlap within a voxel, 
leading to inaccurate quantification of tracer uptake.

*PETPrep* provides a PVC workflow using established tools such as `PETPVC` [Thomas2016]_ or 
`PETsurfer` [Greve2014]_ [Greve2016]_. `PETPVC` provides methods such as ``GTM``, ``LABBE``, ``RL``, ``VC``, 
``RBV``, ``LABBE+RBV``, ``RBV+VC``, ``RBV+RL``, ``LABBE+RBV+VC``, ``LABBE+RBV+RL``,
``STC``, ``MTC``, ``LABBE+MTC``, ``MTC+VC``, ``MTC+RL``, ``LABBE+MTC+VC``,
``LABBE+MTC+RL``, ``IY``, ``IY+VC``, ``IY+RL``, ``MG``, ``MG+VC`` and ``MG+RL``.
`PETsurfer` provides ``GTM``, ``MG``, ``RBV`` and ``AGTM``.

References
----------

.. [Brett2001] Brett M, Leff AP, Rorden C, Ashburner J (2001) Spatial Normalization of Brain Images with
    Focal Lesions Using Cost Function Masking. NeuroImage 14(2)
    doi:`10.006/nimg.2001.0845 <https://doi.org/10.1006/nimg.2001.0845>`_.

.. [Thomas2016]  Thomas, B. A., Cuplov, V., Bousse, A., Mendes, A., Thielemans, K., Hutton, B. F., & 
    Erlandsson, K. (2016). PETPVC: a toolbox for performing partial volume correction techniques in positron 
    emission tomography. Physics in Medicine & Biology, 61(22), 7975.

.. [Greve2014] Greve, D. N., Svarer, C., Fisher, P. M., Feng, L., Hansen, A. E., Baare, W., ... & 
    Knudsen, G. M. (2014). Cortical surface-based analysis reduces bias and variance in kinetic modeling 
    of brain PET data. Neuroimage, 92, 225-236.

.. [Greve2016] Greve, D. N., Salat, D. H., Bowen, S. L., Izquierdo-Garcia, D., Schultz, A. P., 
    Catana, C., ... & Johnson, K. A. (2016). Different partial volume correction methods lead to different 
    conclusions: An 18 F-FDG-PET study of aging. NeuroImage, 132, 334-343.
